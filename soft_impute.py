import numpy as np
import scipy.sparse as sp
from scipy.sparse.linalg import svds
from sklearn.utils.extmath import randomized_svd
import warnings
import time
import os
import gc
import json

warnings.filterwarnings('ignore')


class SoftImputeCV:

    def __init__(self, max_rank=30, lambda_values=None, max_iter=25,
                 convergence_thresh=3e-4, use_randomized_svd=True,
                 svd_iterations=7, random_state=42):
        """
        å‚æ•°:
        - max_rank: æœ€å¤§å¥‡å¼‚å€¼æ•°é‡
        - lambda_values: è¦æµ‹è¯•çš„lambdaå€¼åˆ—è¡¨
        - max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆå¢åŠ åˆ°25æ¬¡ï¼‰
        - convergence_thresh: æ”¶æ•›é˜ˆå€¼ï¼ˆæ”¾å®½åˆ°3e-4ï¼‰
        - use_randomized_svd: æ˜¯å¦ä½¿ç”¨éšæœºSVD
        - svd_iterations: éšæœºSVDçš„è¿­ä»£æ¬¡æ•°
        - random_state: éšæœºç§å­
        """
        self.max_rank = max_rank
        self.lambda_values = lambda_values if lambda_values is not None else [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]
        self.max_iter = max_iter
        self.convergence_thresh = convergence_thresh
        self.use_randomized_svd = use_randomized_svd
        self.svd_iterations = svd_iterations
        self.random_state = random_state

        # ç¼“å­˜
        self.R = None
        self.user_map = None
        self.movie_map = None
        self.global_mean = 0.0
        self.row_means = None
        self.col_means = None

        # ç»“æœå­˜å‚¨
        self.results = {}

        # è®¾ç½®éšæœºç§å­
        np.random.seed(random_state)

    # =========================================================================
    # æ•°æ®åŠ è½½å’Œåˆå§‹åŒ–æ–¹æ³•
    # =========================================================================

    def load_data_sparse(self, filepath='ratings.dat'):
        """åŠ è½½æ•°æ®ä¸ºç¨€ç–çŸ©é˜µ"""
        print("åŠ è½½æ•°æ®ä¸ºç¨€ç–çŸ©é˜µ...")

        cache_file = 'sparse_matrix_cache.npz'
        if os.path.exists(cache_file):
            print("åŠ è½½ç¼“å­˜çš„ç¨€ç–çŸ©é˜µ...")
            self.R = sp.load_npz(cache_file)
            print(f"ä»ç¼“å­˜åŠ è½½: {self.R.shape[0]}ç”¨æˆ·, {self.R.shape[1]}ç”µå½±, {self.R.nnz}è¯„åˆ†")
            return self.R, {}, {}

        # ç¬¬ä¸€æ¬¡éå†ï¼šæ”¶é›†æ‰€æœ‰ID
        user_ids = set()
        movie_ids = set()
        ratings_data = []

        with open(filepath, 'r') as f:
            for line_num, line in enumerate(f):
                if line_num % 1000000 == 0 and line_num > 0:
                    print(f"  å·²å¤„ç† {line_num} è¡Œ...")

                if '::' in line:
                    parts = line.strip().split('::')
                    user_id = int(parts[0])
                    movie_id = int(parts[1])
                    rating = float(parts[2])

                    user_ids.add(user_id)
                    movie_ids.add(movie_id)
                    ratings_data.append((user_id, movie_id, rating))

        # åˆ›å»ºæ˜ å°„
        user_list = sorted(user_ids)
        movie_list = sorted(movie_ids)

        user_map = {uid: idx for idx, uid in enumerate(user_list)}
        movie_map = {mid: idx for idx, mid in enumerate(movie_list)}

        n_users = len(user_list)
        n_movies = len(movie_list)

        print(f"åŸå§‹ç”¨æˆ·æ•°: {max(user_ids)} -> æ˜ å°„å: {n_users}")
        print(f"åŸå§‹ç”µå½±æ•°: {max(movie_ids)} -> æ˜ å°„å: {n_movies}")
        print(f"è¯„åˆ†æ€»æ•°: {len(ratings_data)}")

        # æ„å»ºç¨€ç–çŸ©é˜µ
        rows = np.zeros(len(ratings_data), dtype=np.int32)
        cols = np.zeros(len(ratings_data), dtype=np.int32)
        data = np.zeros(len(ratings_data), dtype=np.float32)

        for i, (user_id, movie_id, rating) in enumerate(ratings_data):
            rows[i] = user_map[user_id]
            cols[i] = movie_map[movie_id]
            data[i] = rating

        R_coo = sp.coo_matrix((data, (rows, cols)), shape=(n_users, n_movies))
        self.R = R_coo.tocsr()

        # ç»Ÿè®¡ä¿¡æ¯
        density = self.R.nnz / (n_users * n_movies) * 100
        print(f"çŸ©é˜µå¯†åº¦: {density:.4f}%")
        print(f"çŸ©é˜µå¤§å°: {n_users}Ã—{n_movies} = {n_users * n_movies:,} å…ƒç´ ")
        print(f"ç¨€ç–çŸ©é˜µå†…å­˜: {self.R.data.nbytes + self.R.indices.nbytes + self.R.indptr.nbytes:,} å­—èŠ‚")

        sp.save_npz(cache_file, self.R)
        print(f"ç¨€ç–çŸ©é˜µå·²ç¼“å­˜åˆ° {cache_file}")

        self.user_map = user_map
        self.movie_map = movie_map

        return self.R, user_map, movie_map

    def compute_matrix_stats(self, R):
        """è®¡ç®—çŸ©é˜µçš„ç»Ÿè®¡ä¿¡æ¯"""
        # å…¨å±€å‡å€¼
        global_mean = np.mean(R.data) if R.nnz > 0 else 3.0
        global_std = np.std(R.data) if R.nnz > 0 else 1.0

        # è¡Œå‡å€¼
        row_sums = np.array(R.sum(axis=1)).flatten()
        row_counts = np.array((R != 0).sum(axis=1)).flatten()
        row_means = np.where(row_counts > 0, row_sums / row_counts, global_mean)

        # åˆ—å‡å€¼
        col_sums = np.array(R.sum(axis=0)).flatten()
        col_counts = np.array((R != 0).sum(axis=0)).flatten()
        col_means = np.where(col_counts > 0, col_sums / col_counts, global_mean)

        return global_mean, global_std, row_means, col_means

    def center_matrix(self, R):
        """ä¸­å¿ƒåŒ–çŸ©é˜µ"""
        n_users, n_movies = R.shape

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        global_mean, global_std, row_means, col_means = self.compute_matrix_stats(R)

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ä»¥ä¾¿åç»­æ¢å¤
        self.global_mean = global_mean
        self.row_means = row_means
        self.col_means = col_means

        # è·å–éé›¶å…ƒç´ çš„ä½ç½®
        rows, cols = R.nonzero()
        data = R.data.copy()

        # ä¸­å¿ƒåŒ–ï¼šç§»é™¤è¡Œåç½®å’Œåˆ—åç½®
        # ä½¿ç”¨åŒé‡ä¸­å¿ƒåŒ–ï¼šX_ij - row_mean_i - col_mean_j + global_mean
        centered_data = data - self.row_means[rows] - self.col_means[cols] + self.global_mean

        # æ„å»ºä¸­å¿ƒåŒ–åçš„ç¨€ç–çŸ©é˜µ
        R_centered = sp.csr_matrix((centered_data, (rows, cols)), shape=(n_users, n_movies))

        # æ£€æŸ¥ä¸­å¿ƒåŒ–åçš„ç»Ÿè®¡ä¿¡æ¯
        if R_centered.nnz > 0:
            centered_mean = np.mean(R_centered.data)
            centered_std = np.std(R_centered.data)
            print(f"ä¸­å¿ƒåŒ–åçŸ©é˜µå‡å€¼: {centered_mean:.6f} (ç›®æ ‡:æ¥è¿‘0)")
            print(f"ä¸­å¿ƒåŒ–åçŸ©é˜µæ ‡å‡†å·®: {centered_std:.3f}")
            print(f"ä¸­å¿ƒåŒ–åæ•°æ®èŒƒå›´: [{R_centered.data.min():.3f}, {R_centered.data.max():.3f}]")

        return R_centered

    def decenter_matrix(self, X_centered):
        """æ¢å¤çŸ©é˜µåˆ°åŸå§‹å°ºåº¦"""
        if self.row_means is None or self.col_means is None:
            return X_centered

        n_users, n_movies = X_centered.shape
        X = X_centered.copy()

        # æ·»åŠ åç½®
        for i in range(n_users):
            X[i, :] += self.row_means[i]

        for j in range(n_movies):
            X[:, j] += self.col_means[j]

        # å‡å»å…¨å±€å‡å€¼ï¼ˆå› ä¸ºåŠ äº†ä¸¤æ¬¡ï¼‰
        X -= self.global_mean

        # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
        X = np.clip(X, 1.0, 5.0)

        return X

    def analyze_singular_values(self, X, max_rank=50):
        """åˆ†æå¥‡å¼‚å€¼ä»¥ç¡®å®šlambdaèŒƒå›´"""
        print("\nåˆ†æå¥‡å¼‚å€¼ä»¥ç¡®å®šåˆé€‚çš„lambdaèŒƒå›´...")

        # è®¡ç®—å¥‡å¼‚å€¼
        if self.use_randomized_svd:
            try:
                _, s, _ = randomized_svd(
                    X.astype(np.float64),
                    n_components=max_rank,
                    n_iter=self.svd_iterations,
                    random_state=self.random_state
                )
                print("ä½¿ç”¨éšæœºSVDè®¡ç®—å¥‡å¼‚å€¼")
            except:
                # å¦‚æœéšæœºSVDå¤±è´¥ï¼Œä½¿ç”¨æˆªæ–­SVD
                _, s, _ = svds(X.astype(np.float64), k=max_rank, which='LM')
                print("ä½¿ç”¨scipyçš„svdsè®¡ç®—å¥‡å¼‚å€¼")
        else:
            _, s, _ = svds(X.astype(np.float64), k=max_rank, which='LM')
            print("ä½¿ç”¨scipyçš„svdsè®¡ç®—å¥‡å¼‚å€¼")

        # æŒ‰é™åºæ’åº
        s = np.sort(s)[::-1]

        # æ‰“å°å¥‡å¼‚å€¼ä¿¡æ¯
        print(f"\nå¥‡å¼‚å€¼åˆ†æ (å‰{len(s)}ä¸ª):")
        print(f"æœ€å¤§å¥‡å¼‚å€¼: {s[0]:.3f}")
        print(f"æœ€å°å¥‡å¼‚å€¼: {s[-1]:.3f}")
        print(f"å¥‡å¼‚å€¼æ€»å’Œ: {np.sum(s):.3f}")
        print(f"å‰10ä¸ªå¥‡å¼‚å€¼: {s[:10]}")

        # è®¡ç®—ç´¯ç§¯èƒ½é‡
        cumulative_energy = np.cumsum(s) / np.sum(s)
        print(f"\nå¥‡å¼‚å€¼ç´¯ç§¯èƒ½é‡:")
        for i in [1, 5, 10, 20, 30, 50]:
            if i <= len(s):
                print(f"  å‰{i}ä¸ªå¥‡å¼‚å€¼å æ€»èƒ½é‡çš„: {cumulative_energy[i - 1]:.1%}")

        # æ ¹æ®å¥‡å¼‚å€¼å¤§å°ç¡®å®šlambdaèŒƒå›´
        max_singular_value = s[0]

        # lambdaåº”è¯¥å¤§çº¦æ˜¯æœ€å¤§å¥‡å¼‚å€¼çš„0.1å€åˆ°10å€
        # ä½†å®é™…ä¸­ï¼Œlambdaé€šå¸¸æ¯”æœ€å¤§å¥‡å¼‚å€¼å°
        lambda_suggestions = []

        # å¦‚æœå¥‡å¼‚å€¼éå¸¸å¤§ï¼Œæˆ‘ä»¬éœ€è¦å¾ˆå¤§çš„lambda
        if max_singular_value > 1000:
            print(f"\nâš ï¸  å¥‡å¼‚å€¼éå¸¸å¤§ (æœ€å¤§å¥‡å¼‚å€¼: {max_singular_value:.1f})")
            print("è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆlambdaéœ€è¦å¾ˆå¤§çš„å€¼")
            # å»ºè®®lambdaèŒƒå›´ï¼šæœ€å¤§å¥‡å¼‚å€¼çš„0.01%åˆ°10%
            lambda_suggestions = [
                max_singular_value * 0.0001,
                max_singular_value * 0.001,
                max_singular_value * 0.01,
                max_singular_value * 0.05,
                max_singular_value * 0.1,
                max_singular_value * 0.5,
                max_singular_value * 1.0,
                max_singular_value * 2.0,
                max_singular_value * 5.0,
                max_singular_value * 10.0
            ]
        elif max_singular_value > 100:
            print(f"\nå¥‡å¼‚å€¼è¾ƒå¤§ (æœ€å¤§å¥‡å¼‚å€¼: {max_singular_value:.1f})")
            # å»ºè®®lambdaèŒƒå›´ï¼šæœ€å¤§å¥‡å¼‚å€¼çš„0.1%åˆ°100%
            lambda_suggestions = [
                max_singular_value * 0.001,
                max_singular_value * 0.01,
                max_singular_value * 0.05,
                max_singular_value * 0.1,
                max_singular_value * 0.5,
                max_singular_value * 1.0,
                max_singular_value * 2.0,
                max_singular_value * 5.0,
                max_singular_value * 10.0,
                max_singular_value * 20.0
            ]
        else:
            print(f"\nå¥‡å¼‚å€¼æ­£å¸¸èŒƒå›´ (æœ€å¤§å¥‡å¼‚å€¼: {max_singular_value:.1f})")
            # å»ºè®®lambdaèŒƒå›´ï¼šæœ€å¤§å¥‡å¼‚å€¼çš„1%åˆ°1000%
            lambda_suggestions = [
                max_singular_value * 0.01,
                max_singular_value * 0.05,
                max_singular_value * 0.1,
                max_singular_value * 0.5,
                max_singular_value * 1.0,
                max_singular_value * 2.0,
                max_singular_value * 5.0,
                max_singular_value * 10.0,
                max_singular_value * 20.0,
                max_singular_value * 50.0
            ]

        # ç¡®ä¿lambdaå€¼éƒ½æ˜¯æ­£æ•°ä¸”åˆç†
        lambda_suggestions = [max(l, 0.001) for l in lambda_suggestions]
        lambda_suggestions = sorted(set(lambda_suggestions))

        print(f"\nå»ºè®®çš„lambdaèŒƒå›´: {[f'{l:.3f}' for l in lambda_suggestions]}")
        print(f"å»ºè®®çš„lambdaèŒƒå›´: {lambda_suggestions}")

        return lambda_suggestions, s

    def initialize_matrix(self, R_centered):
        """åˆå§‹åŒ–ä¸­å¿ƒåŒ–åçš„çŸ©é˜µ"""
        n_users, n_movies = R_centered.shape

        # å¯¹äºä¸­å¿ƒåŒ–åçš„çŸ©é˜µï¼Œåˆå§‹åŒ–ä¸º0çŸ©é˜µï¼Œå¹¶åœ¨è§‚æµ‹ä½ç½®å¡«å……ä¸­å¿ƒåŒ–åçš„å€¼
        X = np.zeros((n_users, n_movies), dtype=np.float32)

        # å¡«å……è§‚æµ‹å€¼
        rows, cols = R_centered.nonzero()
        X[rows, cols] = R_centered.data

        return X

    # =========================================================================
    # Soft-Imputeæ ¸å¿ƒç®—æ³•
    # =========================================================================

    def soft_threshold_svd(self, X, lambda_, k):
        """å¯¹çŸ©é˜µXè¿›è¡Œè½¯é˜ˆå€¼SVD"""
        if self.use_randomized_svd and k > 0:
            U, s, Vt = randomized_svd(
                X.astype(np.float64),
                n_components=k,
                n_iter=self.svd_iterations,
                random_state=self.random_state
            )
        else:
            U, s, Vt = svds(X.astype(np.float64), k=k, which='LM')
            idx = np.argsort(-s)
            s = s[idx]
            U = U[:, idx]
            Vt = Vt[idx, :]

        # è½¯é˜ˆå€¼å¤„ç†
        s_thresh = np.maximum(s - lambda_, 0)

        # é‡å»ºçŸ©é˜µ
        mask = s_thresh > 0
        if np.sum(mask) > 0:
            Z = (U[:, mask] @ np.diag(s_thresh[mask]) @ Vt[mask, :]).astype(np.float32)
        else:
            Z = np.zeros_like(X, dtype=np.float32)

        return Z, s, s_thresh

    def soft_impute_iteration(self, R_train, lambda_):
        """Soft-Imputeè¿­ä»£ï¼ˆ"""
        n_users, n_movies = R_train.shape

        # 1. ä¸­å¿ƒåŒ–è®­ç»ƒé›†
        R_train_centered = self.center_matrix(R_train)

        # è·å–è§‚æµ‹ä½ç½®
        obs_rows, obs_cols = R_train_centered.nonzero()
        obs_data = R_train_centered.data

        # 2. åˆå§‹åŒ–çŸ©é˜µ
        X = self.initialize_matrix(R_train_centered)

        # 3. è¿­ä»£Soft-Impute
        for i in range(self.max_iter):
            start_time = time.time()

            # ç¡®å®šç§©
            k = min(self.max_rank, min(X.shape) - 1)
            if k <= 0:
                k = 1

            try:
                # æ‰§è¡ŒSVD
                Z, s, s_thresh = self.soft_threshold_svd(X, lambda_, k)

                # åˆ›å»ºæ–°çŸ©é˜µ
                X_new = Z.copy()
                X_new[obs_rows, obs_cols] = obs_data

                # é™åˆ¶å€¼èŒƒå›´
                X_new = np.clip(X_new, -10.0, 10.0)  # ä¸­å¿ƒåŒ–åçš„å€¼èŒƒå›´å¯èƒ½æ›´å¹¿

                # æ£€æŸ¥æ”¶æ•›
                change = np.linalg.norm(X_new - X, 'fro') / (np.linalg.norm(X, 'fro') + 1e-10)
                X = X_new

                # è®¡ç®—æœ‰æ•ˆç§©
                effective_rank = np.sum(s_thresh > 0)

                # è®¡ç®—èƒ½é‡ä¿ç•™æ¯”ä¾‹
                if len(s) > 0 and np.sum(s) > 0:
                    energy_retained = np.sum(s_thresh) / np.sum(s)
                else:
                    energy_retained = 0.0

                iter_time = time.time() - start_time

                # æ¯5æ¬¡è¿­ä»£æˆ–æ”¶æ•›æ—¶æ‰“å°ä¿¡æ¯
                if (i + 1) % 5 == 0 or i == 0 or change < self.convergence_thresh:
                    print(f"    è¿­ä»£ {i + 1:2d}/{self.max_iter}: {iter_time:5.1f}ç§’, "
                          f"å˜åŒ–: {change:.6f}, æœ‰æ•ˆç§©: {effective_rank}, "
                          f"èƒ½é‡ä¿ç•™: {energy_retained:.1%}")

                # æ”¶æ•›æ£€æŸ¥
                if change < self.convergence_thresh:
                    if (i + 1) % 5 != 0:  # å¦‚æœè¿˜æ²¡æ‰“å°è¿‡
                        print(f"    è¿­ä»£ {i + 1:2d}/{self.max_iter}: {iter_time:5.1f}ç§’, "
                              f"å˜åŒ–: {change:.6f}, æœ‰æ•ˆç§©: {effective_rank}")
                    print(f"    è¿­ä»£ {i + 1} å·²æ”¶æ•›ï¼ˆå˜åŒ–é‡: {change:.6f} < {self.convergence_thresh}ï¼‰")
                    break

                # æ¸…ç†å†…å­˜
                del Z, s, s_thresh
                gc.collect()

            except MemoryError:
                print(f"    å†…å­˜ä¸è¶³ï¼Œæå‰åœæ­¢è¿­ä»£")
                break
            except Exception as e:
                print(f"    è¿­ä»£å¤±è´¥: {e}")
                break

        # 4. æ¢å¤çŸ©é˜µåˆ°åŸå§‹å°ºåº¦
        X_restored = self.decenter_matrix(X)

        return X_restored

    # =========================================================================
    # äº¤å‰éªŒè¯æ¡†æ¶
    # =========================================================================

    def create_fold(self, R, fold_idx, n_folds=5):
        """åˆ›å»ºäº¤å‰éªŒè¯æŠ˜"""
        fold_seed = self.random_state + fold_idx * 100
        np.random.seed(fold_seed)

        rows, cols = R.nonzero()
        data = R.data
        n_ratings = len(data)

        # æ‰“ä¹±ç´¢å¼•
        indices = np.random.permutation(n_ratings)
        fold_size = n_ratings / n_folds

        # æµ‹è¯•é›†ç´¢å¼•
        test_start = int(fold_idx * fold_size)
        test_end = int((fold_idx + 1) * fold_size) if fold_idx < n_folds - 1 else n_ratings
        test_idx = indices[test_start:test_end]

        # è®­ç»ƒé›†ç´¢å¼•
        train_idx = np.concatenate([indices[:test_start], indices[test_end:]])

        # åˆ›å»ºè®­ç»ƒç¨€ç–çŸ©é˜µ
        train_rows = rows[train_idx]
        train_cols = cols[train_idx]
        train_data = data[train_idx]
        R_train = sp.csr_matrix((train_data, (train_rows, train_cols)), shape=R.shape)

        # æµ‹è¯•é›†ä¿¡æ¯
        test_info = {
            'rows': rows[test_idx],
            'cols': cols[test_idx],
            'true_ratings': data[test_idx]
        }

        return R_train, test_info

    def evaluate_lambda_fold(self, lambda_, fold_idx, R_train, test_info):
        """è¯„ä¼°ç‰¹å®šlambdaåœ¨ç‰¹å®šæŠ˜ä¸Šçš„æ€§èƒ½"""
        print(f"  Î»={lambda_:.3f} - è®­ç»ƒSoft-Impute...")

        # è®­ç»ƒæ¨¡å‹
        X_pred = self.soft_impute_iteration(R_train, lambda_)

        # é¢„æµ‹æµ‹è¯•é›†
        test_rows = test_info['rows']
        test_cols = test_info['cols']
        true_ratings = test_info['true_ratings']

        predicted = X_pred[test_rows, test_cols]

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        rmse = np.sqrt(np.mean((predicted - true_ratings) ** 2))
        mae = np.mean(np.abs(predicted - true_ratings))

        # è£å‰ªé¢„æµ‹å€¼å¹¶é‡æ–°è®¡ç®—
        predicted_clipped = np.clip(predicted, 1.0, 5.0)
        rmse_clipped = np.sqrt(np.mean((predicted_clipped - true_ratings) ** 2))

        print(f"  Î»={lambda_:.3f} - RMSE: {rmse:.6f}, MAE: {mae:.6f}, è£å‰ªåRMSE: {rmse_clipped:.6f}")

        # æ¸…ç†å†…å­˜
        del X_pred
        gc.collect()

        return rmse, mae, rmse_clipped

    def run_single_fold(self, fold_idx, analyze_svd=False):
        """è¿è¡Œå•æŠ˜äº¤å‰éªŒè¯"""
        print(f"\n{'=' * 60}")
        print(f"ç¬¬ {fold_idx + 1}/5 æŠ˜")
        print(f"{'=' * 60}")

        # å¦‚æœæ˜¯ç¬¬ä¸€æŠ˜ï¼ŒåŠ è½½æ•°æ®
        if fold_idx == 0 and self.R is None:
            self.R, self.user_map, self.movie_map = self.load_data_sparse()

        # åˆ›å»ºè®­ç»ƒæµ‹è¯•åˆ†å‰²
        R_train, test_info = self.create_fold(self.R, fold_idx)

        print(f"è®­ç»ƒé›†: {R_train.nnz} è¯„åˆ†")
        print(f"æµ‹è¯•é›†: {len(test_info['true_ratings'])} è¯„åˆ†")

        # å¦‚æœéœ€è¦åˆ†æå¥‡å¼‚å€¼æ¥ç¡®å®šlambdaèŒƒå›´
        if analyze_svd and fold_idx == 0:
            # ä¸­å¿ƒåŒ–è®­ç»ƒé›†
            R_train_centered = self.center_matrix(R_train)
            X_init = self.initialize_matrix(R_train_centered)

            # åˆ†æå¥‡å¼‚å€¼
            suggested_lambdas, singular_values = self.analyze_singular_values(X_init, max_rank=50)

            # ä½¿ç”¨å»ºè®®çš„lambdaå€¼
            self.lambda_values = suggested_lambdas
            print(f"æ ¹æ®å¥‡å¼‚å€¼åˆ†æï¼Œä½¿ç”¨ä»¥ä¸‹lambdaå€¼: {[f'{l:.3f}' for l in self.lambda_values]}")

            # ä¿å­˜å¥‡å¼‚å€¼åˆ†æç»“æœ
            with open('singular_values_analysis.json', 'w') as f:
                json.dump({
                    'singular_values': singular_values.tolist(),
                    'suggested_lambdas': suggested_lambdas,
                    'max_singular_value': float(singular_values[0]),
                    'fold': fold_idx
                }, f, indent=2)
            print("å¥‡å¼‚å€¼åˆ†æç»“æœå·²ä¿å­˜åˆ° singular_values_analysis.json")

        fold_results = []

        # æµ‹è¯•æ‰€æœ‰lambdaå€¼
        for lambda_idx, lambda_ in enumerate(self.lambda_values):
            print(f"\næµ‹è¯• Î»={lambda_:.3f} ({lambda_idx + 1}/{len(self.lambda_values)})")

            fold_start_time = time.time()

            # è¯„ä¼°å½“å‰lambda
            rmse, mae, rmse_clipped = self.evaluate_lambda_fold(lambda_, fold_idx, R_train, test_info)

            fold_time = time.time() - fold_start_time

            # å­˜å‚¨ç»“æœ
            result = {
                'lambda': float(lambda_),
                'fold': fold_idx,
                'rmse': float(rmse),
                'mae': float(mae),
                'rmse_clipped': float(rmse_clipped),
                'time': float(fold_time)
            }

            fold_results.append(result)

            # åˆå§‹åŒ–è¯¥lambdaçš„ç»“æœåˆ—è¡¨
            if lambda_ not in self.results:
                self.results[lambda_] = []
            self.results[lambda_].append(result)

        # æ¸…ç†å†…å­˜
        del R_train
        gc.collect()

        return fold_results

    def run_cross_validation(self, analyze_svd=True):
        """è¿è¡Œäº”æŠ˜äº¤å‰éªŒè¯"""
        print("=" * 70)
        print("ç¨€ç–Soft-Imputeäº”æŠ˜äº¤å‰éªŒè¯ï¼ˆå‚æ•°è°ƒä¼˜ç‰ˆï¼‰")
        print("=" * 70)
        print(f"æµ‹è¯•çš„Î»å€¼: {self.lambda_values}")
        print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.max_iter}")
        print(f"æ”¶æ•›é˜ˆå€¼: {self.convergence_thresh}")
        print(f"ä½¿ç”¨éšæœºSVD: {self.use_randomized_svd}")
        print(f"éšæœºSVDè¿­ä»£æ¬¡æ•°: {self.svd_iterations}")
        print("=" * 70)

        total_start_time = time.time()

        # è¿è¡Œæ‰€æœ‰æŠ˜
        for fold_idx in range(5):
            fold_results = self.run_single_fold(fold_idx, analyze_svd=analyze_svd and fold_idx == 0)

            # ä¿å­˜æ¯æŠ˜çš„ä¸­é—´ç»“æœ
            self.save_intermediate_results(fold_idx, fold_results)

        total_time = time.time() - total_start_time

        # åˆ†æç»“æœ
        best_lambda, best_rmse = self.analyze_results(total_time)

        return best_lambda, best_rmse

    def analyze_results(self, total_time):
        """åˆ†æäº¤å‰éªŒè¯ç»“æœ"""
        print("\n" + "=" * 70)
        print("äº”æŠ˜äº¤å‰éªŒè¯ç»“æœæ±‡æ€»")
        print("=" * 70)

        summary = {}

        for lambda_ in self.lambda_values:
            if lambda_ in self.results and len(self.results[lambda_]) >= 3:
                lambda_results = self.results[lambda_]

                rmses = [r['rmse'] for r in lambda_results]
                rmses_clipped = [r['rmse_clipped'] for r in lambda_results]
                maes = [r['mae'] for r in lambda_results]
                times = [r['time'] for r in lambda_results]

                summary[lambda_] = {
                    'avg_rmse': np.mean(rmses),
                    'std_rmse': np.std(rmses),
                    'avg_rmse_clipped': np.mean(rmses_clipped),
                    'avg_mae': np.mean(maes),
                    'avg_time': np.mean(times),
                    'all_rmses': rmses,
                    'all_maes': maes
                }

        # æ‰“å°è¯¦ç»†ç»“æœè¡¨æ ¼
        print("\nå„Î»å€¼æ€§èƒ½å¯¹æ¯”:")
        print("-" * 90)
        print(
            f"{'Î»':>8} | {'å¹³å‡RMSE':>10} | {'RMSEæ ‡å‡†å·®':>10} | {'è£å‰ªåRMSE':>10} | {'å¹³å‡MAE':>10} | {'å¹³å‡æ—¶é—´(ç§’)':>12}")
        print("-" * 90)

        best_lambda = None
        best_rmse = float('inf')

        for lambda_, stats in sorted(summary.items()):
            print(f"{lambda_:>8.3f} | {stats['avg_rmse']:>10.6f} | {stats['std_rmse']:>10.6f} | "
                  f"{stats['avg_rmse_clipped']:>10.6f} | {stats['avg_mae']:>10.6f} | {stats['avg_time']:>12.1f}")

            if stats['avg_rmse'] < best_rmse:
                best_rmse = stats['avg_rmse']
                best_lambda = lambda_

        print("-" * 90)

        if best_lambda is not None:
            print(f"\nğŸ¯ æœ€ä½³å‚æ•°: Î» = {best_lambda:.3f}")
            print(f"ğŸ¯ æœ€ä½³å¹³å‡RMSE: {best_rmse:.6f}")

            # æ˜¾ç¤ºæœ€ä½³Î»çš„è¯¦ç»†ç»“æœ
            best_stats = summary[best_lambda]
            print(f"å„æŠ˜RMSE: {[f'{r:.6f}' for r in best_stats['all_rmses']]}")
            print(f"å„æŠ˜MAE: {[f'{m:.6f}' for m in best_stats['all_maes']]}")

        print(f"\nâ±ï¸  æ€»è¿è¡Œæ—¶é—´: {total_time:.1f}ç§’ ({total_time / 60:.1f}åˆ†é’Ÿ)")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.save_final_results(summary, best_lambda, best_rmse, total_time)

        # å‚æ•°è°ƒä¼˜å»ºè®®
        print("\n" + "=" * 70)
        print("å‚æ•°è°ƒä¼˜å»ºè®®:")
        print("=" * 70)

        if best_lambda is not None:
            # åˆ†æÎ»çš„è¶‹åŠ¿
            lambdas = sorted(summary.keys())
            avg_rmses = [summary[l]['avg_rmse'] for l in lambdas]

            # å¯»æ‰¾å±€éƒ¨æœ€ä¼˜
            min_idx = np.argmin(avg_rmses)

            print(f"1. æœ€ä½³Î»å€¼åœ¨ {best_lambda:.3f} é™„è¿‘")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å¤§æœç´¢èŒƒå›´
            if min_idx == 0:
                print(f"2. å»ºè®®æµ‹è¯•æ›´å°çš„Î»å€¼ï¼ˆå¦‚ {best_lambda / 2:.3f}ï¼‰")
            elif min_idx == len(lambdas) - 1:
                print(f"2. å»ºè®®æµ‹è¯•æ›´å¤§çš„Î»å€¼ï¼ˆå¦‚ {best_lambda * 2:.3f}ï¼‰")
            else:
                print(f"2. å½“å‰æœç´¢èŒƒå›´å·²åŒ…å«æœ€ä¼˜å€¼")

            # å»ºè®®ç²¾ç»†æœç´¢èŒƒå›´
            left_bound = lambdas[max(0, min_idx - 1)]
            right_bound = lambdas[min(len(lambdas) - 1, min_idx + 1)]
            print(f"3. å»ºè®®åœ¨ [{left_bound:.3f}, {right_bound:.3f}] èŒƒå›´å†…è¿›è¡Œç²¾ç»†æœç´¢")

            # æ ¹æ®æ ‡å‡†å·®æä¾›å»ºè®®
            best_std = summary[best_lambda]['std_rmse']
            if best_std < 0.001:
                print(f"4. ç»“æœç¨³å®šæ€§å¾ˆå¥½ï¼ˆæ ‡å‡†å·®: {best_std:.6f}ï¼‰")
            elif best_std < 0.002:
                print(f"4. ç»“æœç¨³å®šæ€§è¾ƒå¥½ï¼ˆæ ‡å‡†å·®: {best_std:.6f}ï¼‰")
            else:
                print(f"4. ç»“æœç¨³å®šæ€§ä¸€èˆ¬ï¼ˆæ ‡å‡†å·®: {best_std:.6f}ï¼‰ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®")

        return best_lambda, best_rmse

    def save_intermediate_results(self, fold_idx, fold_results):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        filename = f'cv_fold_{fold_idx + 1}_results.json'

        with open(filename, 'w') as f:
            json.dump({
                'fold': fold_idx,
                'parameters': {
                    'max_rank': self.max_rank,
                    'max_iter': self.max_iter,
                    'convergence_thresh': self.convergence_thresh,
                    'use_randomized_svd': self.use_randomized_svd,
                    'svd_iterations': self.svd_iterations
                },
                'results': fold_results
            }, f, indent=2)

        print(f"ç¬¬ {fold_idx + 1} æŠ˜ç»“æœå·²ä¿å­˜åˆ° {filename}")

    def save_final_results(self, summary, best_lambda, best_rmse, total_time):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        # JSONæ ¼å¼
        json_filename = 'softimpute_cv_final_results.json'

        results_dict = {
            'parameters': {
                'max_rank': self.max_rank,
                'lambda_values': self.lambda_values,
                'max_iter': self.max_iter,
                'convergence_thresh': self.convergence_thresh,
                'use_randomized_svd': self.use_randomized_svd,
                'svd_iterations': self.svd_iterations,
                'random_state': self.random_state
            },
            'summary': {str(k): v for k, v in summary.items()},
            'best_parameters': {
                'lambda': float(best_lambda) if best_lambda is not None else None,
                'rmse': float(best_rmse) if best_rmse is not None else None
            },
            'total_time': total_time,
            'all_results': {str(k): v for k, v in self.results.items()}
        }

        with open(json_filename, 'w') as f:
            json.dump(results_dict, f, indent=2)

        # æ–‡æœ¬æ ¼å¼
        txt_filename = 'softimpute_cv_final_results.txt'
        with open(txt_filename, 'w') as f:
            f.write("Soft-Imputeäº”æŠ˜äº¤å‰éªŒè¯æœ€ç»ˆç»“æœ\n")
            f.write("=" * 70 + "\n\n")

            f.write("å‚æ•°è®¾ç½®:\n")
            f.write(f"  æœ€å¤§ç§©: {self.max_rank}\n")
            f.write(f"  æµ‹è¯•Î»å€¼: {self.lambda_values}\n")
            f.write(f"  æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.max_iter}\n")
            f.write(f"  æ”¶æ•›é˜ˆå€¼: {self.convergence_thresh}\n")
            f.write(f"  ä½¿ç”¨éšæœºSVD: {self.use_randomized_svd}\n")
            f.write(f"  éšæœºSVDè¿­ä»£æ¬¡æ•°: {self.svd_iterations}\n\n")

            f.write("å„Î»å€¼æ€§èƒ½æ±‡æ€»:\n")
            f.write("-" * 90 + "\n")
            f.write(
                f"{'Î»':>8} | {'å¹³å‡RMSE':>10} | {'RMSEæ ‡å‡†å·®':>10} | {'è£å‰ªåRMSE':>10} | {'å¹³å‡MAE':>10} | {'å¹³å‡æ—¶é—´(ç§’)':>12}\n")
            f.write("-" * 90 + "\n")

            for lambda_, stats in sorted(summary.items()):
                f.write(f"{lambda_:>8.3f} | {stats['avg_rmse']:>10.6f} | {stats['std_rmse']:>10.6f} | "
                        f"{stats['avg_rmse_clipped']:>10.6f} | {stats['avg_mae']:>10.6f} | {stats['avg_time']:>12.1f}\n")

            f.write("-" * 90 + "\n\n")

            if best_lambda is not None:
                f.write(f"æœ€ä½³å‚æ•°: Î» = {best_lambda:.3f}\n")
                f.write(f"æœ€ä½³å¹³å‡RMSE: {best_rmse:.6f}\n\n")

                best_stats = summary[best_lambda]
                f.write(f"å„æŠ˜è¯¦ç»†ç»“æœ (Î»={best_lambda:.3f}):\n")
                for fold_idx, (rmse, mae) in enumerate(zip(best_stats['all_rmses'], best_stats['all_maes'])):
                    f.write(f"  ç¬¬{fold_idx + 1}æŠ˜: RMSE = {rmse:.6f}, MAE = {mae:.6f}\n")

            f.write(f"\næ€»è¿è¡Œæ—¶é—´: {total_time:.1f}ç§’ ({total_time / 60:.1f}åˆ†é’Ÿ)\n")

        print(f"\nğŸ“ æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  ğŸ“„ JSONæ ¼å¼: {json_filename}")
        print(f"  ğŸ“„ æ–‡æœ¬æ ¼å¼: {txt_filename}")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    print("=" * 70)
    print("ç¨€ç–Soft-Imputeäº”æŠ˜äº¤å‰éªŒè¯ï¼ˆå‚æ•°è°ƒä¼˜ç‰ˆï¼‰")
    print("=" * 70)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists('ratings.dat'):
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° ratings.dat æ–‡ä»¶")
        print("è¯·å°† ratings.dat æ–‡ä»¶æ”¾åœ¨å½“å‰ç›®å½•")
        exit(1)

    # åˆå§‹lambdaå€¼ï¼ˆæ ¹æ®å¥‡å¼‚å€¼åˆ†æåä¼šè¢«è°ƒæ•´ï¼‰
    lambda_values = [
        0.01,  # éå¸¸å°çš„æ­£åˆ™åŒ–
        0.05,  # è¾ƒå°çš„æ­£åˆ™åŒ–
        0.1,  # ä¸­ç­‰æ­£åˆ™åŒ–
        0.5,  # è¾ƒå¼ºçš„æ­£åˆ™åŒ–
        1.0,  # å¼ºæ­£åˆ™åŒ–
        2.0,  # å¾ˆå¼ºçš„æ­£åˆ™åŒ–
        5.0  # æå¼ºçš„æ­£åˆ™åŒ–
    ]

    # åˆ›å»ºå‚æ•°è°ƒä¼˜æ¨¡å‹
    model = SoftImputeCV(
        max_rank=20,  # å¥‡å¼‚å€¼æ•°é‡ï¼ˆä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼‰
        lambda_values=lambda_values,  # åˆå§‹lambdaå€¼ï¼Œä¼šæ ¹æ®å¥‡å¼‚å€¼åˆ†æè°ƒæ•´
        max_iter=25,  # å¢åŠ åˆ°25æ¬¡è¿­ä»£
        convergence_thresh=3e-4,  # æ”¾å®½æ”¶æ•›é˜ˆå€¼
        use_randomized_svd=True,  # ä½¿ç”¨éšæœºSVDåŠ é€Ÿ
        svd_iterations=7,  # éšæœºSVDè¿­ä»£æ¬¡æ•°ï¼ˆå¢åŠ ä»¥æé«˜ç²¾åº¦ï¼‰
        random_state=42  # å›ºå®šéšæœºç§å­
    )

    try:
        print("ğŸš€ å¼€å§‹å‚æ•°è°ƒä¼˜äº¤å‰éªŒè¯...")
        print(f"ğŸ“Š å°†æ ¹æ®å¥‡å¼‚å€¼åˆ†æç¡®å®šlambdaèŒƒå›´")
        print(f"ğŸ”„ æ¯ä¸ªÎ»å€¼å°†è¿è¡Œäº”æŠ˜äº¤å‰éªŒè¯")
        print(f"â±ï¸  é¢„è®¡æ€»æ—¶é—´: çº¦3-4å°æ—¶")
        print("=" * 70)

        # è¿è¡Œäº¤å‰éªŒè¯ï¼Œå¹¶åˆ†æå¥‡å¼‚å€¼æ¥ç¡®å®šlambdaèŒƒå›´
        best_lambda, best_rmse = model.run_cross_validation(analyze_svd=True)

        print("\n" + "=" * 70)
        print("âœ… äº¤å‰éªŒè¯å®Œæˆ!")
        print("=" * 70)

        if best_lambda is not None:
            print(f"\nğŸ¯ æœ€ç»ˆæ¨èå‚æ•°:")
            print(f"   Î» = {best_lambda:.3f}")
            print(f"   RMSE = {best_rmse:.6f}")

    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰ç»“æœ...")
        if hasattr(model, 'results') and model.results:
            with open('interrupted_results.json', 'w') as f:
                json.dump({str(k): v for k, v in model.results.items()}, f, indent=2)
            print("ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜åˆ° interrupted_results.json")
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    print("\nğŸ ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()
