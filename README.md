# 基于凸方法和非凸方法的矩阵填充实验报告

---

## 一、研究背景与数据集说明

在现代推荐系统中，用户–物品评分矩阵通常具有两个显著特征：

1. **高度稀疏**：绝大多数用户仅对极少数物品进行过评分；
2. **潜在低秩结构**：用户偏好与物品属性可由少量潜在因子刻画。

矩阵填充问题正是基于上述特征提出，其核心目标是在**仅观测部分矩阵元素**的条件下，恢复完整评分矩阵。

**MovieLens 10M**数据集是推荐系统研究中广泛使用的基准数据集之一，包含来自约71,567名匿名用户对10,677部电影的1,000万条评分记录（评分范围为1-5星），构成了一个极端稀疏的用户-电影评分矩阵（密度仅为约1.31%），该数据集具有真实用户行为的长尾分布和时序特征，常被用于评估协同过滤与矩阵补全算法的性能，为研究提供了规模适中且具有代表性的实验数据基础。

本实验基于此数据集，分别采用：

- soft-impute的凸方法
- AM的非凸方法

并通过 **5 折交叉验证** 比较两种方法的表现。

---

## 二、问题建模与算法理论

### 2.1 矩阵补全问题形式化

设 $M \in \mathbb{R}^{n \times m}$ 为用户-电影评分矩阵，其中 $n$ 为用户数量，$m$ 为电影数量。矩阵元素 $$M_{ij}$$ 表示用户 $i$ 对电影 $j$ 的评分（1-5分），但仅在索引集合 $$\Omega$$ 上可观测，其余元素缺失。矩阵补全问题旨在利用观测到的部分评分，预测缺失的评分值。

该问题可以形式化为以下带秩约束的优化问题：

$$\[
\min_{X} \sum_{(i,j) \in \Omega} (X_{ij} - M_{ij})^2 \quad \text{s.t.} \quad \text{rank}(X) \leq r
\]$$

其中 $X$ 为待补全的矩阵， $r$ 为预设的秩上限。秩约束是非凸的，直接求解困难。

### 2.2 Soft-Impute 方法

给定一个稀疏矩阵 $R \in \mathbb{R}^{m \times n}$，其中包含部分观测值，Soft-Impute旨在找到一个低秩矩阵 $X$，最小化以下目标函数：

$$
\min_X \frac{1}{2} \|P_\Omega(X - R)\|_F^2 + \lambda \|X\|_*
$$

其中：
- $$P_\Omega$$ 是观测位置的投影算子
- $$\|\cdot\|_F$$ 是Frobenius范数
- $$\|\cdot\|_*$$ 是核范数（奇异值之和）
- $$\lambda$$ 是正则化参数

**优化过程**：
算法采用迭代软阈值SVD方法：

1. 初始化： $$X^{(0)} = P_\Omega(R)$$ 
2. 迭代更新：  $$X^{(k+1)} = S_\lambda\left(X^{(k)} + P_\Omega(R - X^{(k)})\right)$$
   
   其中 $$S_\lambda(Z) = U \cdot \text{diag}((\sigma_i - \lambda)_+) \cdot V^T$$ 是软阈值算子

4. 收敛条件： $$\|X^{(k+1)} - X^{(k)}\|_F / \|X^{(k)}\|_F < \epsilon$$ 

### 2.3 AM 方法

交替最小化（Alternating Minimization, AM）方法通过在固定其中一个因子矩阵的条件下，交替优化另一个因子矩阵，将原始的非凸优化问题分解为一系列带正则化的最小二乘子问题。

模型目标函数形式如下：

$$
\min_{U,V} \sum_{(u,i)\in\Omega} \left(r_{ui} - U_u^\top V_i\right)^2 + \lambda \left( \|U\|_F^2 + \|V\|_F^2 \right)
$$

其中， $U \in \mathbb{R}^{n \times r}$ 与 $V \in \mathbb{R}^{m \times r}$ 分别表示用户与物品的潜在因子矩阵， $r$ 为矩阵分解秩（rank）， $\lambda$ 为正则化参数。
 
---

## 三、实验设计

### 3.1 Soft-Impute 方法

#### 数据预处理

1. **数据读取与缓存**：
   - 首次加载时解析`ratings.dat`文件，建立用户ID与电影ID的连续索引映射
   - 将评分数据转换为CSR格式稀疏矩阵并缓存至`sparse_matrix_cache.npz`
   - 后续运行直接加载缓存文件，减少重复计算

2. **矩阵统计量计算**：
   - 全局均值（`global_mean`）：所有评分的平均值
   - 用户均值（`row_means`）：每个用户的平均评分
   - 电影均值（`col_means`）：每部电影的平均评分

3. **矩阵中心化**：
      - 计算用户平均评分： $$r_i = \frac{1}{n_i} \sum_{j \in \Omega_i} R_{ij}$$ 
   - 计算电影平均评分： $$c_j = \frac{1}{m_j} \sum_{i \in \Omega_j} R_{ij}$$ 
   - 中心化： $$R_{ij}^c = R_{ij} - r_i - c_j + \mu$$ 
   - 其中 $$\mu$$ 是全局平均评分

#### 参数设置

奇异值分析
1. 对中心化后的矩阵进行SVD分解（支持随机SVD加速）
2. 计算奇异值累积能量分布，评估矩阵低秩特性
3. 根据最大奇异值动态确定正则化参数λ的候选范围：
   - 最大奇异值>1000：λ范围为最大奇异值的0.01%~10%
   - 100<最大奇异值≤1000：λ范围为最大奇异值的0.1%~100%
   - 最大奇异值≤100：λ范围为最大奇异值的1%~1000%

基于奇异值分析和实验经验确定参数：

| 参数 | 设置值 | 选择依据 |
|------|--------|----------|
| **最大秩（max_rank）** | 20 | 根据奇异值分析，前20个奇异值累积能量占比超过66.9%，能够捕获大部分数据变异 |
| **λ搜索范围** | 基于奇异值分析自动确定 | 根据最大奇异值（288.271）的比例确定，覆盖从0.1%到2000%的范围，确保搜索充分 |
| **最大迭代次数（max_iter）** | 25 | 实验发现迭代20-25次已充分收敛，增加次数对性能提升有限 |
| **收敛阈值（convergence_thresh）** | 3e-4 | 平衡精度和计算效率，更小的阈值会增加计算时间但改进有限 |
| **使用随机SVD（use_randomized_svd）** | True | 处理大规模矩阵（69,878×10,677）时，随机SVD比传统SVD快10倍以上 |
| **随机SVD迭代次数（svd_iterations）** | 7 | 提供足够的精度，同时保持计算效率 |
| **随机种子（random_state）** | 42 | 确保结果可重复性 |

#### 交叉验证方案

采用五折交叉验证评估模型性能：
- 将评分数据随机分为5等份
- 每次使用4份作为训练集，1份作为测试集
- 重复5次，计算平均性能指标

#### 评估指标

1. **均方根误差（RMSE）**：   $$\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2}$$

3. **平均绝对误差（MAE）**：  $$\text{MAE} = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|$$

5. **裁剪后RMSE**：将预测值裁剪到[1.0, 5.0]区间后重新计算RMSE

---

### 3.2 AM 方法（Alternating Minimization）

#### 方法概述

本实验采用 **Alternating Minimization (AM)** 算法对评分矩阵进行矩阵分解，同时考虑 **全局均值、用户偏置和物品偏置**，模型公式如下：

$$
r_{ui} \approx \mu + b_u + b_i + U_u^\top V_i
$$

其中：
- $\mu$：评分全局均值  
- $b_u$：用户偏置  
- $b_i$：物品偏置  
- $U \in \mathbb{R}^{n_\text{users}\times r}, V \in \mathbb{R}^{n_\text{items}\times r}$：潜在因子矩阵  

偏置更新公式为：

$$
b_i = \frac{\sum_{u\in \mathcal{U}_i} (r_{ui} - \mu - b_u - U_u^\top V_i)}{|\mathcal{U}_i| + \lambda_\text{bias}}, \quad
b_u = \frac{\sum_{i\in \mathcal{I}_u} (r_{ui} - \mu - b_i - U_u^\top V_i)}{|\mathcal{I}_u| + \lambda_\text{bias}}
$$

潜在因子更新使用 **减去偏置后的残差** 进行最小二乘求解，并加入 $\ell_2$ 正则：

$$
U_u = \arg\min_{U_u} \sum_{i \in \mathcal{I}_u} \big(r_{ui} - \mu - b_u - b_i - U_u^\top V_i\big)^2 + \lambda_\text{reg} \|U_u\|_2^2
$$

$$
V_i = \arg\min_{V_i} \sum_{u \in \mathcal{U}_i} \big(r_{ui} - \mu - b_u - b_i - U_u^\top V_i\big)^2 + \lambda_\text{reg} \|V_i\|_2^2
$$


#### 参数设置

1. **矩阵秩（rank）**

矩阵秩 $r$ 决定了潜在因子空间的维度，是 AM 方法中最关键的超参数之一。本实验在初步对比的基础上，重点考察了 $r=20$ 与 $r=50$ 两种设置。

实验结果表明：

- 当 $r=20$ 时，模型在测试集上取得了较低的 RMSE（约 **0.89**），且收敛速度较快；
- 当 $r=50$ 时，模型复杂度显著增加，但测试集 RMSE 不降反升，呈现出一定过拟合迹象。

综合预测性能与计算代价，本文最终选择：

$$
r = 20
$$

作为 AM 方法的统一秩设置。


2. **正则化参数（ $\lambda$ ）**

$\lambda_\text{reg} = 0.1$  
$\lambda_\text{bias} = 10.0$  


3. **自适应收敛条件**

为避免固定迭代次数带来的冗余计算，本文在 AM 算法中引入**基于训练误差变化的自适应收敛准则**。

在第 $t$ 次迭代结束后，若训练集 RMSE 的相对变化满足：

$$
\frac{|\mathrm{RMSE}_t - \mathrm{RMSE}_{t-1}|}{\mathrm{RMSE}_{t-1}} < \varepsilon,
$$

则认为算法已基本收敛并提前终止。

实验中设置：

$$
\varepsilon = 10^{-3}
$$

同时，为保证算法在极端情况下仍能停止，设置最大迭代次数上限为 **50 次**。


**AM 方法与 Soft-Impute 方法采用完全一致的 5 折交叉验证方案和评价指标，此处不再赘述**

   
---


## 四、实验结果及分析

### 4.1 Soft-Impute 方法

#### 奇异值分析

通过对训练矩阵进行奇异值分解，得到以下关键信息：
- **最大奇异值**：288.271（表明数据变化幅度较大）
- **奇异值衰减**：前50个奇异值占总能量的100%
- **确定 $$\lambda$$ 范围**：根据最大奇异值的大小，将 $$\lambda$$ 设置为最大奇异值的不同比例（0.1%-2000%）

#### 不同 $$\lambda$$ 值的性能比较

从五折交叉验证结果可以看出：

|  $\lambda$  | 平均RMSE | RMSE标准差 | 裁剪后RMSE | 平均MAE | 平均时间(秒) |
|-----------|----------|------------|------------|---------|--------------|
| 0.288 | 0.811944 | 0.000453 | 0.811944 | 0.617626 | 841.3 |
| 2.883 | 0.811306 | 0.000457 | 0.811306 | 0.617332 | 830.7 |
| **14.414** | **0.809567** | **0.000474** | **0.809567** | **0.616817** | 839.2 |
| 28.827 | 0.809938 | 0.000492 | 0.809938 | 0.618010 | 844.1 |
| 144.136 | 0.859031 | 0.000524 | 0.859031 | 0.660845 | 839.9 |
| ≥288.271 | 0.885061 | 0.000534 | 0.885061 | 0.682678 | ~42.0 |

**关键发现**：

1. **最优 $$\lambda$$ 值**： $$\lambda = 14.414$$ 时取得最佳RMSE（0.809567）
2. **性能趋势**：
   - 当 $$\lambda$$ 较小时（0.288-28.827），模型性能相近
   - 当 $$\lambda$$ 达到144.136时，性能显著下降（RMSE增加约6%）
   - 当 $$\lambda \geq 288.271$$ 时，模型完全收缩为零矩阵（有效秩为0）
3. **时间效率**：大 $$\lambda$$ 值收敛极快（仅需1次迭代），但性能较差

#### 收敛行为分析

观察不同 $$\lambda$$ 值的迭代过程：

1. **小 $$\lambda$$ 值（<30）**：
   - 有效秩保持为20（最大设置值）
   - 能量保留率>90%
   - 需要25次迭代达到收敛
   - 变化量逐渐减小

2. **中等 $$\lambda$$ 值（144.136）**：
   - 有效秩降至7-10
   - 能量保留率约30%
   - 仍需要多次迭代收敛

3. **大 $$\lambda$$ 值（≥288.271）**：
   - 有效秩为0（所有奇异值被截断）
   - 一次迭代即收敛
   - 预测结果为偏置矩阵（行均值+列均值-全局均值）

#### 模型稳定性

- **低标准差**：各 $$\lambda$$ 值的RMSE标准差在0.0005以内，表明五折交叉验证结果稳定
- **性能一致性**：各折的RMSE值非常接近，最小0.808771，最大0.812554

---

### 4.2 AM 方法

#### 4.2.1 实验结果
| Fold | RMSE   | Iterations | Training Time (s) |
|------|--------|------------|-----------------|
| 1    | 0.9681 | 20         | 467.08          |
| 2    | 0.9721 | 21         | 466.61          |
| 3    | 0.9698 | 20         | 1023.17         |
| 4    | 0.9678 | 20         | 1192.87         |
| 5    | 0.9699 | 20         | 1337.40         |

- **平均 RMSE**: 0.9695  
- **平均迭代次数**: 20.2  
- **平均训练时间**: 897.43 s  

训练过程中，AM 在大约 20 次迭代后收敛，训练 RMSE 相对稳定。

#### 4.2.2 结果分析

1. **Bias 校正效果**  
   - AM 采用全局均值 $\mu$ 和用户/物品偏置 $b_u, b_i$ 校正评分，使潜在因子专注拟合残差，减轻偏置对因子估计的干扰。

2. **RMSE 对比**  
   - 本实验中，AM 的 5 折平均 RMSE = 0.9695，而 **soft-impute** 平均 RMSE 略低（约 0.95）。  
   - 原因：
     1. AM 固定 $\lambda_\text{reg}$，未进行超参数调优  
     2. soft-impute 对整个评分矩阵进行低秩近似，全局拟合能力更强  
     3. AM 收敛到局部最优，潜在因子可能未完全捕捉全局低秩特性

3. **算法优劣**
   
| 算法         | 优势                                   | 劣势                     |
|--------------|--------------------------------------|-------------------------|
| AM           | 可解释性强，潜在因子拟合残差稳定       | RMSE 略高，训练时间长   |
| soft-impute  | 全局低秩拟合能力强，RMSE 更低          | 未显式分离 bias，可解释性略差 |


---

## 五、结论与讨论：Soft-Impute 与 AM 的比较


### 5.1 算法性能对比

| 方法 | 测试集平均 RMSE | 平均训练时间（s） | 平均迭代次数 | 优劣分析 |
|------|----------------|-----------------|-------------|----------|
| Soft-Impute | 0.809567 | 839.2 | 25 | RMSE 更低，训练效率较高，模型稳定性好 |
| AM | 0.9695 | 897.43 | 20.2 | RMSE 较高，训练时间更长，需进一步优化参数 |

- **RMSE 对比**：Soft-Impute 在本实验中表现明显优于 AM 方法，说明凸优化的核范数正则化在捕捉矩阵低秩结构上更有效。  
- **训练效率**：两种方法时间差距不大，但 Soft-Impute 在迭代次数与收敛速度上略占优势。  
- **模型稳定性**：Soft-Impute 在五折交叉验证中标准差更低，预测结果更一致；AM 在训练过程中收敛稳定，但最终测试性能略逊。

### 5.2 AM 方法分析

1. **固定 λ 的限制**：本实验中 AM 使用固定 λ=0.1，未进行网格搜索或交叉验证优化，可能导致潜在因子过拟合或欠拟合，影响测试 RMSE。  
2. **非凸性影响**：AM 是非凸优化，可能收敛到局部最优，尤其在高维稀疏矩阵下更容易受到初始值和迭代顺序影响。  
3. **收敛阈值设置**：自适应收敛准则 tol=1e-3 能保证训练收敛，但未必能找到全局最优，可能略高于最优 RMSE。

### 5.3 Soft-Impute 优势原因

- **凸优化问题**：Soft-Impute 的核范数正则化形成凸优化问题，保证了全局收敛性。  
- **奇异值软阈值**：通过软阈值截断奇异值，有效剔除噪声和高阶特征，使模型更稳健。  
- **参数调优充分**：实验中对 λ 值进行了系统搜索，从而获得最优 RMSE。

### 5.4 改进建议

1. **AM 参数调优**：
   - 对 λ 进行网格搜索或交叉验证，寻找最优正则化强度；
   - 考虑对 rank 进行探索，动态选择潜在因子维度。
2. **初始化策略优化**：
   - 采用 SVD 或其他启发式方法初始化 U、V，减少陷入局部最优的概率。  
3. **结合 Soft-Impute 思路**：
   - 可在 AM 中引入奇异值截断或核范数约束，提高算法稳健性。  
4. **并行化与加速**：
   - 对大规模矩阵分解任务，可使用随机分块或 GPU 加速，降低训练时间。

### 5.5 总结

- Soft-Impute 在本实验中整体表现优于 AM 方法，主要得益于凸优化全局收敛性和系统的 λ 参数搜索。  
- AM 方法仍具有灵活性和扩展性，但当前实验设置下测试性能略逊。通过参数调优、初始化改进和正则化增强，AM 的表现有望接近或超过 Soft-Impute。  

